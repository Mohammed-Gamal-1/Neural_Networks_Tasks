# Task 09 â€” Activation Function Variants

## Objective
Compare different activation functions.

## Results
ReLU and GELU perform better than Tanh and Softsign.

## Analysis
ReLU avoids vanishing gradients.
GELU provides smooth activation behavior.

## Key Takeaway
Activation choice affects gradient flow and training stability.
