# Task 07 â€” Optimizer Comparison Challenge

## Objective
Compare different optimizers and study their effect on training.

## Results
Adam converges faster than SGD.
SGD with momentum performs better than plain SGD.
AdamW shows better generalization.

## Analysis
Different optimizers update weights in different ways.
Adaptive optimizers adjust learning rates automatically.
Weight decay in AdamW helps reduce overfitting.

## Key Takeaway
Optimizer choice strongly affects training speed and stability.
